True Positive (TP): Correctly predicted positive cases.
True Negative (TN): Correctly predicted negative cases.
False Positive (FP): Incorrectly predicted positive cases.
False Negative (FN): Incorrectly predicted negative cases.
-------------------------------------------------------------------

Precision:
how consistent or close repeated measurements or predictions are to each other.
Ex. If you throw darts and they all land close together (even if far from the bullseye),
your throws are precise but not accurate. If your darts land near the bullseye, they are
accurate. If they also land close together, they are both precise and accurate.
In classification problems, precision is a metric used to measure how many of the
positive predictions made by a model are actually correct. It focuses on the quality of positive predictions.

Precision = TP / (TP + FP)

Ex. you have test 100 emails for spam, you correctly identify 40 as spam (TP) and
incorrectly identify 10 as spam when they are not (FP).
Your precision is 40 / (40 + 10) = 0.8 or 80%.
This means that 80% of the emails you predicted as spam were actually spam.
Precision answers the question: "Of all the emails flagged as spam, how many are actually spam?"
In this example:
80% precision means that 80% of the emails flagged as spam are indeed spam,
while 20% are incorrectly flagged (false positives).
Precision is especially important in situations where false positives are costly or
harmful, such as: fraud detection, medical diagnoses, and spam filtering.

----------------------------------------------------------------------

Accuracy: how close a measurement or prediction is to the true or actual value.
Ex. If you throw darts and they all land near the bullseye, your throws are accurate.
It measures the proportion of correct predictions (both positive and negative) out of
all predictions made by the model.

Accuracy = (TP + TN) / (TP + TN + FP + FN)

90% accuracy means that the model correctly predicted 90% of the samples
(both positive and negative).
However, accuracy alone may not always be a reliable metric, especially for
imbalanced datasets (e.g., when one class is much larger than the other).
Accuracy is a good metric when the dataset is balanced
(i.e., both classes have similar proportions).
For imbalanced datasets, consider other metrics like precision, recall, or F1-score to better evaluate the model's performance.

--------------------------------------------------------------------------










In statistics, **precision** refers to the level of consistency or repeatability of a measurement or estimate. It reflects how much variability exists in repeated measurements or predictions. Precision is a measure of **reliability**, not accuracy (which measures how close a result is to the true value).

---

### **Key Concepts of Precision in Statistics**

1. **Precision vs. Accuracy**:
   - **Precision**: How consistent or close repeated measurements or predictions are to each other.
   - **Accuracy**: How close a measurement or prediction is to the true or actual value.
   - Example:
     - If you throw darts and they all land close together (even if far from the bullseye), your throws are **precise** but not **accurate**.
     - If your darts land near the bullseye, they are **accurate**. If they also land close together, they are both **precise** and **accurate**.

2. **In Relation to Variability**:
   - High precision means low variability in measurements or predictions.
   - Low precision means high variability, where repeated measurements or predictions are spread out.

3. **In Statistical Models**:
   - Precision is often related to the standard error or confidence intervals of an estimate.
   - A smaller standard error or narrower confidence interval indicates higher precision.

---

### **Precision in Classification Problems (Machine Learning/Statistics)**

In the context of classification problems (e.g., binary classification), **precision** is a specific metric used to evaluate the performance of a model. It measures how many of the predicted positive results are actually correct.

#### Formula for Precision:
\[
{Precision} = {{True Positives (TP)}}{{True Positives (TP)} + \{False Positives (FP)}}
\]

- **True Positives (TP)**: Cases where the model correctly predicted the positive class.
- **False Positives (FP)**: Cases where the model incorrectly predicted the positive class.

#### Interpretation:
- Precision answers the question: **"Of all the positive predictions made by the model, how many were actually correct?"**
- It is particularly important in cases where **false positives** are costly (e.g., fraud detection, medical diagnoses).

#### Example:
If a model predicts 100 positive cases, and 80 of them are correct (true positives), while 20 are incorrect (false positives):
\[
\text{Precision} = \frac{80}{80 + 20} = 0.8 \, (80\%)
\]

---

### **Real-Life Applications of Precision**
1. **Medical Testing**:
   - In a diagnostic test, precision measures how many of the people who tested positive actually have the disease.
   - High precision is important to avoid unnecessary treatments for people who don’t actually have the condition.

2. **Spam Detection**:
   - Precision measures how many of the emails flagged as spam are truly spam.
   - High precision minimizes the risk of marking important emails as spam.

3. **Search Engines**:
   - Precision measures how many of the retrieved search results are relevant to the query.

---

### **Improving Precision**
- Reduce false positives by making the decision threshold stricter.
- Use more reliable or specific features in your model to better distinguish between classes.

---
#########
### **False Negative Explained Simply**

A **false negative** happens when something is wrongly labeled as **negative**, even though it is actually **positive**.

#### **Example (Super Simple):**
Imagine a fire alarm:
- If there’s a fire, but the alarm doesn’t go off, that’s a **false negative**.

The system says **“No fire”**, but there **is a fire**.

---

### **Why It’s Important**
False negatives are bad because they **miss something important**:
- **Medical Tests**: A false negative means the test says you're healthy, but you're actually sick.
- **Fraud Detection**: A false negative means fraud happens, but no one notices.

### **How is a False Negative Calculated?**

In classification problems (like predicting yes/no or positive/negative), a **false negative (FN)** is calculated as part of the confusion matrix, which includes the following terms:

1. **True Positive (TP)**: Correctly predicted positive cases.
2. **True Negative (TN)**: Correctly predicted negative cases.
3. **False Positive (FP)**: Incorrectly predicted positive cases.
4. **False Negative (FN)**: Incorrectly predicted negative cases.

---

### **False Negative Count**
The **false negative count** is simply the number of cases where:
- The actual (true) value is **positive**.
- The predicted value is **negative**.

#### **Formula**:
There’s no specific formula for calculating false negatives directly, but they are identified by comparing predictions with actual values in the dataset.

---

### **Example of Calculation**
Imagine you have a dataset with 10 cases. The actual and predicted values are:

| Case | Actual (True) | Predicted |
|------|---------------|-----------|
| 1    | Positive      | Positive  |
| 2    | Positive      | Negative  | ← **False Negative**
| 3    | Negative      | Negative  |
| 4    | Positive      | Positive  |
| 5    | Negative      | Negative  |
| 6    | Positive      | Negative  | ← **False Negative**
| 7    | Negative      | Negative  |
| 8    | Positive      | Positive  |
| 9    | Negative      | Positive  |
| 10   | Positive      | Negative  | ← **False Negative**

In this case:
- **False Negatives (FN)** = 3 (Cases 2, 6, and 10).

---

### **False Negative Rate (FNR)**
You can calculate the **False Negative Rate (FNR)**, which tells you the proportion of actual positives that were incorrectly classified as negatives.

#### **Formula**:
\[
\text{FNR} = \frac{\text{False Negatives (FN)}}{\text{False Negatives (FN)} + \text{True Positives (TP)}}
\]

---

### **Example Calculation of FNR**
Using the table above:
- **False Negatives (FN)** = 3
- **True Positives (TP)** = 4 (Cases 1, 4, 8)

\[
\text{FNR} = \frac{3}{3 + 4} = \frac{3}{7} \approx 0.4286 \, (42.86\%)
\]

This means that 42.86% of the actual positive cases were incorrectly classified as negative.

---

##############################
### **How is a False Positive (FP) Calculated?**

In classification problems (like predicting yes/no or positive/negative outcomes), a **false positive (FP)** is calculated as part of the **confusion matrix**, which includes the following terms:

1. **True Positive (TP)**: Correctly predicted positive cases.
2. **True Negative (TN)**: Correctly predicted negative cases.
3. **False Positive (FP)**: Incorrectly predicted positive cases.
4. **False Negative (FN)**: Incorrectly predicted negative cases.

---

### **False Positive Count**
The **false positive count** is simply the number of cases where:
- The actual (true) value is **negative**.
- The predicted value is **positive**.

#### **How to Identify False Positives**:
Compare the actual and predicted values in the dataset:
- If the actual value is **negative**, but the prediction is **positive**, it is a **false positive**.

---

### **Example of Calculation**

Imagine you have a dataset with 10 cases. The actual and predicted values are:

| Case | Actual (True) | Predicted |
|------|---------------|-----------|
| 1    | Positive      | Positive  |
| 2    | Positive      | Negative  |
| 3    | Negative      | Negative  |
| 4    | Positive      | Positive  |
| 5    | Negative      | Negative  |
| 6    | Positive      | Negative  |
| 7    | Negative      | Negative  |
| 8    | Positive      | Positive  |
| 9    | Negative      | Positive  | ← **False Positive**
| 10   | Negative      | Positive  | ← **False Positive**

In this case:
- **False Positives (FP)** = 2 (Cases 9 and 10).

---

### **False Positive Rate (FPR)**
You can calculate the **False Positive Rate (FPR)**, which tells you the proportion of actual negatives that were incorrectly classified as positives.

#### **Formula**:
\[
\text{FPR} = \frac{\text{False Positives (FP)}}{\text{False Positives (FP)} + \text{True Negatives (TN)}}
\]

---

### **Example Calculation of FPR**
Using the table above:
- **False Positives (FP)** = 2
- **True Negatives (TN)** = 3 (Cases 3, 5, and 7)

\[
\text{FPR} = \frac{2}{2 + 3} = \frac{2}{5} = 0.4 \, (40\%)
\]

This means that 40% of the actual negative cases were incorrectly classified as positive.

---

### **Key Notes**
1. **False Positives Matter**:
   - False positives can lead to unnecessary actions or decisions (e.g., unnecessary medical treatments, false alarms in security systems).

2. **Reducing False Positives**:
   - Adjusting the decision threshold can reduce false positives, but it may increase false negatives.

####################################

### **What is the F1 Score?**

The **F1 Score** is a metric used to evaluate the balance between **precision** and **recall** in a classification model. It combines both metrics into a single score to give you a sense of how well your model performs, especially when there is an imbalance between precision and recall.

---

### **Simple Explanation**

The F1 Score answers the question:

> **"How well does the model balance correctly identifying positive cases (precision) and capturing all actual positives (recall)?"**

It is particularly useful when you want to find a balance between:
- **Precision**: How many of the predicted positives are correct.
- **Recall**: How many of the actual positives were correctly identified.

---

### **The Formula for F1 Score**
The F1 Score is the **harmonic mean** of precision and recall:

\[
\text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\]

Where:
- **Precision** = \(\frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}\)
- **Recall** = \(\frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}\)

---

### **Why Use the Harmonic Mean?**
The harmonic mean gives more weight to the smaller value of precision or recall. This means that the F1 Score will only be high if **both precision and recall are high**. If one of them is very low, the F1 Score will also be low.

---

### **Example**

Imagine you're building a spam email classifier. Here's the performance of your model:

- **True Positives (TP)**: 70 (spam emails correctly identified as spam)
- **False Positives (FP)**: 30 (non-spam emails incorrectly identified as spam)
- **False Negatives (FN)**: 20 (spam emails missed by the model)

#### **Step 1: Calculate Precision**
\[
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} = \frac{70}{70 + 30} = 0.7 \, (70\%)
\]

#### **Step 2: Calculate Recall**
\[
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{70}{70 + 20} = 0.777 \, (77.7\%)
\]

#### **Step 3: Calculate F1 Score**
\[
\text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\]
\[
\text{F1 Score} = 2 \times \frac{0.7 \times 0.777}{0.7 + 0.777} = 2 \times \frac{0.5439}{1.477} = 0.735 \, (73.5\%)
\]

---

### **Interpretation**
- The F1 Score of **73.5%** means that the model has a reasonably good balance between precision and recall.
- If either precision or recall were much lower, the F1 Score would drop significantly.

---

### **When is the F1 Score Useful?**
- **Imbalanced Datasets**: When one class (e.g., spam vs. non-spam) is much larger than the other, accuracy alone can be misleading. The F1 Score provides a better measure of performance in such cases.
- **When Both Precision and Recall Matter**: For example:
  - **Medical Diagnosis**: You want to find as many sick patients as possible (high recall) without falsely diagnosing healthy patients (high precision).
  - **Fraud Detection**: You want to catch fraudulent transactions (high recall) while minimizing false alarms (high precision).

---

================================

### **What is ROC AUC?**

**ROC AUC** stands for **Receiver Operating Characteristic - Area Under the Curve**. It is a metric used to evaluate the performance of a **binary classification model** by measuring its ability to distinguish between the two classes (e.g., positive and negative).

---

### **Simple Explanation**

1. **ROC Curve**:
   - The **ROC Curve** is a graph that shows the trade-off between:
     - **True Positive Rate (TPR)** (also called **Recall**): How many actual positives the model correctly identifies.
     - **False Positive Rate (FPR)**: How many actual negatives the model incorrectly identifies as positive.
   - The curve is created by plotting the TPR against the FPR at different thresholds.

2. **AUC (Area Under the Curve)**:
   - The **AUC** is the area under the ROC Curve.
   - It provides a single number to summarize the model's performance.
   - AUC ranges from 0 to 1:
     - **1.0 (100%)**: Perfect model (always distinguishes between positive and negative correctly).
     - **0.5 (50%)**: Random guessing (no discriminatory power).
     - **< 0.5**: Worse than random guessing.

---

### **How to Interpret ROC AUC**

- **High AUC (close to 1.0)**:
  - The model is good at distinguishing between the two classes.
  - For example, if AUC = 0.95, the model has a **95% chance** of correctly ranking a randomly chosen positive instance higher than a randomly chosen negative instance.

- **AUC = 0.5**:
  - The model is no better than random guessing (e.g., flipping a coin).

- **AUC < 0.5**:
  - The model is worse than random guessing, which might indicate a problem with the model or data.

---

### **Example**

Imagine you’re building a model to detect fraud in transactions (fraud = positive, not fraud = negative):

| Threshold | True Positive Rate (TPR) | False Positive Rate (FPR) |
|-----------|---------------------------|---------------------------|
| 0.2       | 0.95                      | 0.50                      |
| 0.5       | 0.80                      | 0.20                      |
| 0.8       | 0.60                      | 0.05                      |

#### **ROC Curve**:
- Plot TPR (y-axis) vs. FPR (x-axis) for each threshold.
- The curve shows how the model performs as the threshold changes.

#### **AUC**:
- The area under this curve is the **ROC AUC**.
- If AUC = 0.90, the model is very good at distinguishing between fraud and non-fraud.

---

### **When to Use ROC AUC**
- **Binary Classification**:
  - It’s most commonly used for binary classification tasks (e.g., spam vs. not spam, fraud vs. no fraud).
- **Imbalanced Datasets**:
  - ROC AUC is useful when the dataset is imbalanced because it evaluates the model’s ability to rank predictions correctly, regardless of class imbalance.

---

### **Key Points**
1. **Thresholds**:
   - The ROC Curve shows how the model performs at different classification thresholds.
   - A threshold determines when the model classifies something as positive or negative (e.g., probability > 0.5 = positive).

2. **Trade-Off**:
   - A good model balances **True Positives** (catching positives) and **False Positives** (avoiding false alarms).

3. **AUC as a Single Metric**:
   - The AUC condenses the performance of the model across all thresholds into a single number.

---

### **Summary**
- **ROC Curve**: A graph showing the relationship between TPR and FPR at different thresholds.
- **AUC (Area Under the Curve)**: A single score summarizing the model's ability to distinguish between classes.
- **High AUC**: Indicates a better-performing model, with 1.0 being perfect and 0.5 being random guessing.

---
########################################
